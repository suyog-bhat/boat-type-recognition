# -*- coding: utf-8 -*-
"""boat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tnoxVf0QDK8dD8lCKTpn306FwCUt8IeO
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import os
import cv2 
import keras
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Flatten, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dropout
from keras.layers.core import Dense
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from PIL import Image
import numpy as np

os.chdir('./boats')

os.listdir()

DATA_DIR = "." #because already chdir-ed into the  directory 
BATCH_SIZE = 64
img_height, img_width = 224, 224;

Datagen = ImageDataGenerator(rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2) # set validation split


train_generator = Datagen.flow_from_directory(
    DATA_DIR,
    target_size=(img_height, img_width),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training') # set as training data

validation_generator = Datagen.flow_from_directory(
    DATA_DIR,
    target_size=(img_height, img_width),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation') # set as validation data

model = keras.Sequential()

# 1st Convolutional Layer
model.add(Conv2D(filters=128, input_shape=(224,224,3), kernel_size=(3,3), padding='valid'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
model.add(BatchNormalization())

# 2nd Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3), padding='valid'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))
model.add(BatchNormalization())

# 3rd Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), padding='valid'))
model.add(Activation('relu'))
model.add(BatchNormalization())

# 4th Convolutional Layer
model.add(Conv2D(filters=384, kernel_size=(3,3), padding='valid'))
model.add(Activation('relu'))
model.add(BatchNormalization())

# 5th Convolutional Layer
model.add(Conv2D(filters=256, kernel_size=(3,3), padding='valid'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))
model.add(BatchNormalization())

model.add(Flatten())
# 1st Dense Layer
model.add(Dense(4096))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())

# 2nd Dense Layer
model.add(Dense(4096))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())

# 3rd Dense Layer
model.add(Dense(1000))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(BatchNormalization())

# Prediction Layer
model.add(Dense(9))
model.add(Activation('softmax'))

checkpoint = ModelCheckpoint("./top.h5", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)
early = EarlyStopping(monitor='val_acc', min_delta=0, patience=15, verbose=1, mode='auto')

modell = tf.keras.models.load_model('./top.h5')
modell.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

modell.fit(
train_generator,
steps_per_epoch = 1175//BATCH_SIZE ,
epochs = 50,
validation_data = validation_generator,
callbacks = [checkpoint])


modell=load_model('./top.h5')
modell.evaluate(validation_generator)

